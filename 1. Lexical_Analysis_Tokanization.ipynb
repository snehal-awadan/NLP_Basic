{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35a2ab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\rohit\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\rohit\\anaconda3\\lib\\site-packages (3.7.4)\n",
      "Requirement already satisfied: textblob in c:\\users\\rohit\\anaconda3\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: click in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk spacy textblob -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea9a147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2963eb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package indian to\n",
      "[nltk_data]     C:\\Users\\Rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package indian is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')       #tokenization\n",
    "nltk.download(\"stopwords\")   #stopwords removel\n",
    "nltk.download(\"averaged_perceptron_tagger\")#POS tagging\n",
    "nltk.download(\"wordnet\")     #wordnet database and lemmatization\n",
    "nltk.download(\"omw-1.4\")     #stemming\n",
    "nltk.download(\"indian\")      #indian language POS tagging\n",
    "nltk.download(\"maxent_ne_chunker\")#chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d29c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one sentence:\n",
    "\n",
    "sent = 'They told that their ages are 25 26 and 31 respectively'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b21c487f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.333333333333332"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the average of the given ages:\n",
    "\n",
    "ages = []\n",
    "\n",
    "# split the words\n",
    "for word in sent.split():\n",
    "    \n",
    "    # if the word is digit\n",
    "    if word.isdigit():\n",
    "        \n",
    "        # then convert into int and then append\n",
    "        ages.append(int(word))\n",
    "        \n",
    "# find the average:\n",
    "\n",
    "sum(ages)/len(ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47b8c184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.333333333333332"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average using np:\n",
    "\n",
    "np.mean([int(word) for word in sent.split() if word.isdigit()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0246b94",
   "metadata": {},
   "source": [
    "# word and sent Tokanization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98fecb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one sentence:\n",
    "\n",
    "sent=\"Hello friends! How are you? Welcome to python Programming.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7414729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4843bf4e",
   "metadata": {},
   "source": [
    "apply sent_tokenize:\n",
    "it will break down the words if it encounter any punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7781705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!', 'How are you?', 'Welcome to python Programming.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4110361",
   "metadata": {},
   "source": [
    "apply word_tokenize:\n",
    "it will break down the every words including punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffc38c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'python',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bef497f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the %  punctuation symbol present in sent:\n",
    "\n",
    "pun_per = len([word  for word in word_tokenize(sent) if not word.isalnum()])\n",
    "\n",
    "pun_per/len(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80e2dadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n",
      "35\n",
      "115\n",
      "2357\n"
     ]
    }
   ],
   "source": [
    "# find the ascii value:\n",
    "\n",
    "print(ord('y'))\n",
    "print(ord('#'))\n",
    "print(ord('s'))\n",
    "print(ord('à¤µ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a73a3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the size of the any symbol and alphabet\n",
    "# initialy all the size will be 50 only\n",
    "\n",
    "import sys\n",
    "\n",
    "#help(sys.getsizeof)\n",
    "\n",
    "sys.getsizeof('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4e9cbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50 + (no.of char-1)\n",
    "\n",
    "char = 'dsgdbvcbf'\n",
    "\n",
    "sys.getsizeof(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "780230e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U\n",
      "u\n",
      "à “\n",
      "à¤µ\n",
      "à¤µà¥€\n",
      "à¤µ\n"
     ]
    }
   ],
   "source": [
    "# get the alphabet from the ascii value:\n",
    "\n",
    "print(chr(85))\n",
    "\n",
    "print(chr(117))\n",
    "\n",
    "print(chr(2067))\n",
    "\n",
    "print(\"\\u0935\")\n",
    "\n",
    "print(\"\\u0935\\u0940\")\n",
    "\n",
    "print(chr(0x935))      #hex\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e4cd4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "name = \"à¤¸à¥à¤¨à¥‡à¤¹à¤²\"\n",
    "\n",
    "print(name.startswith('à¤¸à¥'))\n",
    "\n",
    "print(name.find('à¤²'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9c0acb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¤°à¥à¤¥\n",
      "à¤°à¥à¤¤à¥à¤œà¤¾\n"
     ]
    }
   ],
   "source": [
    "names = [\"à¤¸à¥à¤¨à¥‡à¤¹à¤²\" ,\"à¤®à¤¿à¤¤à¤²à¥€\", \"à¤¶à¥à¤µà¥‡à¤¤à¤¾\",  \"à¤°à¥à¤¥\", \"à¤°à¥à¤¤à¥à¤œà¤¾\"]\n",
    "\n",
    "for name in names:\n",
    "    if name.startswith('à¤°à¥'):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75ab241b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['à¤°à¤¾à¤œà¤—à¤¡', 'à¤¹à¤¾', 'à¤­à¤¾à¤°à¤¤à¤¾à¤šà¥à¤¯à¤¾', 'à¤®à¤¹à¤¾à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°', 'à¤°à¤¾à¤œà¥à¤¯à¤¾à¤¤à¥€à¤²', 'à¤à¤•', 'à¤•à¤¿à¤²à¥à¤²à¤¾', 'à¤†à¤¹à¥‡.', 'à¤°à¤¾à¤œà¤—à¤¡', 'Archived', '2020-08-05', 'at', 'the', 'Wayback', 'Machine.', 'à¤•à¤¿à¤²à¥à¤²à¥à¤¯à¤¾à¤µà¤°', 'à¤›à¤¤à¥à¤°à¤ªà¤¤à¥€', 'à¤¶à¤¿à¤µà¤¾à¤œà¥€', 'à¤®à¤¹à¤¾à¤°à¤¾à¤œ', 'à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾', 'à¤¸à¥à¤µà¤°à¤¾à¤œà¥à¤¯à¤¾à¤šà¥€', 'à¤ªà¤¹à¤¿à¤²à¥€', 'à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€', 'à¤¹à¥‹à¤¤à¥€.', 'à¤ªà¥à¤£à¥‡', 'à¤¶à¤¹à¤°à¤¾à¤šà¥à¤¯à¤¾', 'à¤¨à¥ˆà¤‹à¤¤à¥à¤¯à¥‡à¤²à¤¾']\n",
      "\n",
      "after applying word_tokenize:\n",
      "\n",
      "['à¤°à¤¾à¤œà¤—à¤¡', 'à¤¹à¤¾', 'à¤­à¤¾à¤°à¤¤à¤¾à¤šà¥à¤¯à¤¾', 'à¤®à¤¹à¤¾à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°', 'à¤°à¤¾à¤œà¥à¤¯à¤¾à¤¤à¥€à¤²', 'à¤à¤•', 'à¤•à¤¿à¤²à¥à¤²à¤¾', 'à¤†à¤¹à¥‡', '.', 'à¤°à¤¾à¤œà¤—à¤¡', 'Archived', '2020-08-05', 'at', 'the', 'Wayback', 'Machine', '.', 'à¤•à¤¿à¤²à¥à¤²à¥à¤¯à¤¾à¤µà¤°', 'à¤›à¤¤à¥à¤°à¤ªà¤¤à¥€', 'à¤¶à¤¿à¤µà¤¾à¤œà¥€', 'à¤®à¤¹à¤¾à¤°à¤¾à¤œ', 'à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾', 'à¤¸à¥à¤µà¤°à¤¾à¤œà¥à¤¯à¤¾à¤šà¥€', 'à¤ªà¤¹à¤¿à¤²à¥€', 'à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€', 'à¤¹à¥‹à¤¤à¥€', '.', 'à¤ªà¥à¤£à¥‡', 'à¤¶à¤¹à¤°à¤¾à¤šà¥à¤¯à¤¾', 'à¤¨à¥ˆà¤‹à¤¤à¥à¤¯à¥‡à¤²à¤¾']\n"
     ]
    }
   ],
   "source": [
    "# create one para in marathi:\n",
    "\n",
    "mtext = 'à¤°à¤¾à¤œà¤—à¤¡ à¤¹à¤¾ à¤­à¤¾à¤°à¤¤à¤¾à¤šà¥à¤¯à¤¾ à¤®à¤¹à¤¾à¤°à¤¾à¤·à¥à¤Ÿà¥à¤° à¤°à¤¾à¤œà¥à¤¯à¤¾à¤¤à¥€à¤² à¤à¤• à¤•à¤¿à¤²à¥à¤²à¤¾ à¤†à¤¹à¥‡. à¤°à¤¾à¤œà¤—à¤¡ Archived 2020-08-05 at the Wayback Machine. à¤•à¤¿à¤²à¥à¤²à¥à¤¯à¤¾à¤µà¤° à¤›à¤¤à¥à¤°à¤ªà¤¤à¥€ à¤¶à¤¿à¤µà¤¾à¤œà¥€ à¤®à¤¹à¤¾à¤°à¤¾à¤œ à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤¸à¥à¤µà¤°à¤¾à¤œà¥à¤¯à¤¾à¤šà¥€ à¤ªà¤¹à¤¿à¤²à¥€ à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€ à¤¹à¥‹à¤¤à¥€. à¤ªà¥à¤£à¥‡ à¤¶à¤¹à¤°à¤¾à¤šà¥à¤¯à¤¾ à¤¨à¥ˆà¤‹à¤¤à¥à¤¯à¥‡à¤²à¤¾'\n",
    "\n",
    "# split the para:\n",
    "\n",
    "print(mtext.split())\n",
    "\n",
    "print()\n",
    "\n",
    "# apply the word_tokenize:\n",
    "print('after applying word_tokenize:\\n')\n",
    "print(word_tokenize(mtext))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84f485f",
   "metadata": {},
   "source": [
    "# Space Tokenize"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c245a2d",
   "metadata": {},
   "source": [
    "create one text file which contains some words and save it as'space_mydata.txt'.\n",
    "it will print the content which present in the txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9ff4aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello FriendsÃ°Å¸Ëœâ‚¬!\\tHow are youÃ°Å¸Â«\\xa0?\\nWelcomeÃ°Å¸Â«Â° to the worldÃ°Å¸Å’Å½ of\\tPythonÃ¢â„¢â€ ProgrammingÃ°Å¸ËœÂ·.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'space_mydata.txt'\n",
    "\n",
    "open(path).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d45f3930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(path).readable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e9fb79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello FriendsÃ°Å¸Ëœâ‚¬!\\tHow are youÃ°Å¸Â«\\xa0?\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(path).readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "263a29c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello FriendsÃ°Å¸Ëœâ‚¬!\\tHow are youÃ°Å¸Â«\\xa0?\\n',\n",
       " 'WelcomeÃ°Å¸Â«Â° to the worldÃ°Å¸Å’Å½ of\\tPythonÃ¢â„¢â€ ProgrammingÃ°Å¸ËœÂ·.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(path).readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92ed32f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello FriendsÃ°Å¸Ëœâ‚¬!\\tHow are youÃ°Å¸Â«\\xa0?\\nWelcomeÃ°Å¸Â«Â° to the worldÃ°Å¸Å’Å½ of\\tPythonÃ¢â„¢â€ ProgrammingÃ°Å¸ËœÂ·.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open the file and read the data from it:\n",
    "\n",
    "file = open('space_mydata.txt')\n",
    "\n",
    "data = file.read()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d85da76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'FriendsÃ°Å¸Ëœâ‚¬!\\tHow',\n",
       " 'are',\n",
       " 'youÃ°Å¸Â«\\xa0?\\nWelcomeÃ°Å¸Â«Â°',\n",
       " 'to',\n",
       " 'the',\n",
       " 'worldÃ°Å¸Å’Å½',\n",
       " 'of\\tPythonÃ¢â„¢â€',\n",
       " 'ProgrammingÃ°Å¸ËœÂ·.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import SpaceTokenizer\n",
    "\n",
    "tk = SpaceTokenizer()\n",
    "\n",
    "# it will break down the word if it encounter the space\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dbd001",
   "metadata": {},
   "source": [
    "# Tab Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4b13081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello FriendsÃ°Å¸Ëœâ‚¬!',\n",
       " 'How are youÃ°Å¸Â«\\xa0?\\nWelcomeÃ°Å¸Â«Â° to the worldÃ°Å¸Å’Å½ of',\n",
       " 'PythonÃ¢â„¢â€ ProgrammingÃ°Å¸ËœÂ·.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TabTokenizer\n",
    "\n",
    "tk = TabTokenizer()\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9def62ea",
   "metadata": {},
   "source": [
    "# Line Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d8a401d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello FriendsÃ°Å¸Ëœâ‚¬!\\tHow are youÃ°Å¸Â«\\xa0?',\n",
       " 'WelcomeÃ°Å¸Â«Â° to the worldÃ°Å¸Å’Å½ of\\tPythonÃ¢â„¢â€ ProgrammingÃ°Å¸ËœÂ·.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import LineTokenizer\n",
    "\n",
    "lt = LineTokenizer()\n",
    "lt.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba056b34",
   "metadata": {},
   "source": [
    "# Whitespace Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f05b0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'FriendsÃ°Å¸Ëœâ‚¬!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'youÃ°Å¸Â«',\n",
       " '?',\n",
       " 'WelcomeÃ°Å¸Â«Â°',\n",
       " 'to',\n",
       " 'the',\n",
       " 'worldÃ°Å¸Å’Å½',\n",
       " 'of',\n",
       " 'PythonÃ¢â„¢â€',\n",
       " 'ProgrammingÃ°Å¸ËœÂ·.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "wt = WhitespaceTokenizer()\n",
    "wt.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cbc7fb",
   "metadata": {},
   "source": [
    "# Multi Word Expression (MWD) Tokenize"
   ]
  },
  {
   "cell_type": "raw",
   "id": "161706a5",
   "metadata": {},
   "source": [
    "It will break down every alphabet present in the sentence\n",
    "including space and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e995d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020 The Van Rossum is python creator, visiting Pune this week. The development community is very eager to meet Van Rossum.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sent1 = '2020 The Van Rossum is python creator, visiting Pune this week. The development community is very eager to meet Van Rossum.'\n",
    "sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "481ae32b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " ' ',\n",
       " 'T',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'V',\n",
       " 'a',\n",
       " 'n',\n",
       " ' ',\n",
       " 'R',\n",
       " 'o',\n",
       " 's',\n",
       " 's',\n",
       " 'u',\n",
       " 'm',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'p',\n",
       " 'y',\n",
       " 't',\n",
       " 'h',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 'c',\n",
       " 'r',\n",
       " 'e',\n",
       " 'a',\n",
       " 't',\n",
       " 'o',\n",
       " 'r',\n",
       " ',',\n",
       " ' ',\n",
       " 'v',\n",
       " 'i',\n",
       " 's',\n",
       " 'i',\n",
       " 't',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'P',\n",
       " 'u',\n",
       " 'n',\n",
       " 'e',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'w',\n",
       " 'e',\n",
       " 'e',\n",
       " 'k',\n",
       " '.',\n",
       " ' ',\n",
       " 'T',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " 'v',\n",
       " 'e',\n",
       " 'l',\n",
       " 'o',\n",
       " 'p',\n",
       " 'm',\n",
       " 'e',\n",
       " 'n',\n",
       " 't',\n",
       " ' ',\n",
       " 'c',\n",
       " 'o',\n",
       " 'm',\n",
       " 'm',\n",
       " 'u',\n",
       " 'n',\n",
       " 'i',\n",
       " 't',\n",
       " 'y',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'v',\n",
       " 'e',\n",
       " 'r',\n",
       " 'y',\n",
       " ' ',\n",
       " 'e',\n",
       " 'a',\n",
       " 'g',\n",
       " 'e',\n",
       " 'r',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'm',\n",
       " 'e',\n",
       " 'e',\n",
       " 't',\n",
       " ' ',\n",
       " 'V',\n",
       " 'a',\n",
       " 'n',\n",
       " ' ',\n",
       " 'R',\n",
       " 'o',\n",
       " 's',\n",
       " 's',\n",
       " 'u',\n",
       " 'm',\n",
       " '.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "mwd = MWETokenizer()\n",
    "mwd.tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "373ad2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020',\n",
       " 'The',\n",
       " 'Van Rossum',\n",
       " 'is',\n",
       " 'python',\n",
       " 'creator',\n",
       " ',',\n",
       " 'visiting',\n",
       " 'Pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'The',\n",
       " 'development',\n",
       " 'community',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'Van Rossum',\n",
       " '.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk = MWETokenizer(separator=' ')\n",
    "\n",
    "tk.add_mwe(('Van','Rossum'))\n",
    "\n",
    "#tokenize the data\n",
    "tk.tokenize(word_tokenize(sent1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a579308d",
   "metadata": {},
   "source": [
    "# Tweet Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8981da2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hello Friends :) ! How are you :( ? Welcome to the world of Python Programming. :D\n"
     ]
    }
   ],
   "source": [
    "sent=\"'Hello Friends :) ! How are you :( ? Welcome to the world of Python Programming. :D\"\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43cbb37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\",\n",
       " 'Hello',\n",
       " 'Friends',\n",
       " ':)',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " ':(',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.',\n",
       " ':D']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tw = TweetTokenizer()\n",
    "\n",
    "tw.tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a1e4c842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello FriendsðŸ˜€!\tHow are youðŸ« ?\n",
      "WelcomeðŸ«° to the worldðŸŒŽ of\tPythonâ™” ProgrammingðŸ˜·.\n"
     ]
    }
   ],
   "source": [
    "f=open('space_mydata.txt', encoding='utf-8')\n",
    "data = f.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "216445d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'FriendsðŸ˜€',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'youðŸ« ',\n",
       " '?',\n",
       " 'WelcomeðŸ«°',\n",
       " 'to',\n",
       " 'the',\n",
       " 'worldðŸŒŽ',\n",
       " 'of',\n",
       " 'Pythonâ™”',\n",
       " 'ProgrammingðŸ˜·',\n",
       " '.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aede18de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " 'ðŸ˜€',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " 'ðŸ« ',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'ðŸ«°',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'ðŸŒŽ',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'â™”',\n",
       " 'Programming',\n",
       " 'ðŸ˜·',\n",
       " '.']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeebcc54",
   "metadata": {},
   "source": [
    "# Custom Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "34652cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: \n",
      "\n",
      "This\n",
      "is\n",
      "some\n",
      "text\n",
      "punctuation\n",
      ">\n",
      "Let's\n",
      "tokenize\n",
      "it\n",
      "Is\n",
      "it\n",
      "ok\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# regular expression:\n",
    "import re\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    \n",
    "    return re.split(r'[.,;?!\\s]+',text)\n",
    "\n",
    "# create text:\n",
    "text = \"This is some text punctuation > Let's tokenize it. Is it ok?\"\n",
    "\n",
    "# call function:\n",
    "tokens = custom_tokenizer(text)\n",
    "\n",
    "print('Tokens: \\n')\n",
    "\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "100e454b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roll\tname\tclass\tmarks\tage\n",
      "1\tanil\tTE\t56.77\t22\n",
      "2\tamit\tTE\t59.77\t21\n",
      "3\taniket\tBE\t76.88\t19\n",
      "4\tajinkya\tTE\t69.66\t20\n",
      "5\tasha\tTE\t63.28\t20\n",
      "6\tayesha\tBE\t49.55\t20\n",
      "7\tamar\tBE\t65.34\t19\n",
      "8\tamita\tBE\t68.33\t23\n",
      "9\tamol\tTE\t56.75\t20\n",
      "10\tanmol\tBE\t78.66\t21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open('student3.csv')\n",
    "data = f.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "530fd094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.split('\\n')\n",
    "\n",
    "# lst = []\n",
    "\n",
    "# for x in data:\n",
    "    \n",
    "#     inner_lst = []\n",
    "    \n",
    "#     for y in x.split('\\t'):\n",
    "#         if (y.isdigit()):\n",
    "#             inner_lst.append(int(y))\n",
    "            \n",
    "#         elif y.find('.') > 0:\n",
    "#             inner_lst.append(float(y))\n",
    "            \n",
    "#         else:\n",
    "#             inner_lst.append(y)\n",
    "            \n",
    "#     lst.append(inner_lst)\n",
    "# lst.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458f7a84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
